{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/workspace/python\n",
      "Collecting tensorflow==1.7.1 (from unityagents==0.4.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/66/83/35c3f53129dfc80d65ebbe07ef0575263c3c05cc37f8c713674dcedcea6f/tensorflow-1.7.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: Pillow>=4.2.1 in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (5.2.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (1.12.1)\n",
      "Collecting jupyter (from unityagents==0.4.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/83/df/0f5dd132200728a86190397e1ea87cd76244e42d39ec5e88efd25b2abd7e/jupyter-1.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pytest>=3.2.2 in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (4.5.0)\n",
      "Collecting docopt (from unityagents==0.4.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (3.12)\n",
      "Requirement already satisfied: protobuf==3.5.2 in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (3.5.2)\n",
      "Requirement already satisfied: grpcio==1.11.0 in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (1.11.0)\n",
      "Requirement already satisfied: torch==0.4.0 in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (0.23.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (1.2.1)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (4.9.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.7.1->unityagents==0.4.0) (1.11.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.7.1->unityagents==0.4.0) (0.5.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.7.1->unityagents==0.4.0) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<1.8.0,>=1.7.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.7.1->unityagents==0.4.0) (1.7.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.7.1->unityagents==0.4.0) (0.30.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.7.1->unityagents==0.4.0) (0.8.1)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.7.1->unityagents==0.4.0) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil>=2.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib->unityagents==0.4.0) (2.6.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from matplotlib->unityagents==0.4.0) (2017.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib->unityagents==0.4.0) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->unityagents==0.4.0) (2.2.0)\n",
      "Collecting qtconsole (from jupyter->unityagents==0.4.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/3a/57/c8fc1fc6fb6bc03caca20ace9cd0ac0e16cc052b51cbe3acbeeb53abcb18/qtconsole-5.1.1-py3-none-any.whl\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.6/site-packages (from jupyter->unityagents==0.4.0) (5.7.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.6/site-packages (from jupyter->unityagents==0.4.0) (7.0.5)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.6/site-packages (from jupyter->unityagents==0.4.0) (5.4.0)\n",
      "Collecting jupyter-console (from jupyter->unityagents==0.4.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/59/cd/aa2670ffc99eb3e5bbe2294c71e4bf46a9804af4f378d09d7a8950996c9b/jupyter_console-6.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from pytest>=3.2.2->unityagents==0.4.0) (0.1.7)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /opt/conda/lib/python3.6/site-packages (from pytest>=3.2.2->unityagents==0.4.0) (1.3.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from pytest>=3.2.2->unityagents==0.4.0) (1.8.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from pytest>=3.2.2->unityagents==0.4.0) (38.4.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0; python_version > \"2.7\" in /opt/conda/lib/python3.6/site-packages (from pytest>=3.2.2->unityagents==0.4.0) (7.0.0)\n",
      "Requirement already satisfied: pluggy!=0.10,<1.0,>=0.9 in /opt/conda/lib/python3.6/site-packages (from pytest>=3.2.2->unityagents==0.4.0) (0.11.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from pytest>=3.2.2->unityagents==0.4.0) (19.1.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from ipykernel->unityagents==0.4.0) (6.5.0)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in /opt/conda/lib/python3.6/site-packages (from ipykernel->unityagents==0.4.0) (4.3.2)\n",
      "Requirement already satisfied: jupyter_client in /opt/conda/lib/python3.6/site-packages (from ipykernel->unityagents==0.4.0) (5.2.4)\n",
      "Requirement already satisfied: tornado>=4.0 in /opt/conda/lib/python3.6/site-packages (from ipykernel->unityagents==0.4.0) (4.5.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.1->unityagents==0.4.0) (0.14.1)\n",
      "Requirement already satisfied: bleach==1.5.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.1->unityagents==0.4.0) (1.5.0)\n",
      "Requirement already satisfied: html5lib==0.9999999 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.1->unityagents==0.4.0) (0.9999999)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.1->unityagents==0.4.0) (2.6.9)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.6/site-packages (from qtconsole->jupyter->unityagents==0.4.0) (4.4.0)\n",
      "Collecting qtpy (from qtconsole->jupyter->unityagents==0.4.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/27/54/459e3aa002eefff68c7d16babb3ab6bc1e4c2106e70654fbb5a8788d45be/QtPy-1.11.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.6/site-packages (from qtconsole->jupyter->unityagents==0.4.0) (2.2.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.6/site-packages (from qtconsole->jupyter->unityagents==0.4.0) (0.2.0)\n",
      "Requirement already satisfied: pyzmq>=17.1 in /opt/conda/lib/python3.6/site-packages (from qtconsole->jupyter->unityagents==0.4.0) (17.1.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.6/site-packages (from notebook->jupyter->unityagents==0.4.0) (2.10)\n",
      "Requirement already satisfied: nbformat in /opt/conda/lib/python3.6/site-packages (from notebook->jupyter->unityagents==0.4.0) (4.4.0)\n",
      "Requirement already satisfied: Send2Trash in /opt/conda/lib/python3.6/site-packages (from notebook->jupyter->unityagents==0.4.0) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /opt/conda/lib/python3.6/site-packages (from notebook->jupyter->unityagents==0.4.0) (0.8.1)\n",
      "Requirement already satisfied: prometheus_client in /opt/conda/lib/python3.6/site-packages (from notebook->jupyter->unityagents==0.4.0) (0.3.1)\n",
      "Collecting widgetsnbextension~=3.0.0 (from ipywidgets->jupyter->unityagents==0.4.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/8d/f2/c8bcccccbed39d51d3e237fb0c0f0c9bbc845d12afc41f5ca5f5728fffc7/widgetsnbextension-3.0.8-py2.py3-none-any.whl\n",
      "Requirement already satisfied: mistune>=0.8.1 in /opt/conda/lib/python3.6/site-packages (from nbconvert->jupyter->unityagents==0.4.0) (0.8.3)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from nbconvert->jupyter->unityagents==0.4.0) (0.2.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.6/site-packages (from nbconvert->jupyter->unityagents==0.4.0) (1.4.1)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.6/site-packages (from nbconvert->jupyter->unityagents==0.4.0) (0.3.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.6/site-packages (from nbconvert->jupyter->unityagents==0.4.0) (0.5.0)\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 (from jupyter-console->jupyter->unityagents==0.4.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/c6/37/ec72228971dbaf191243b8ee383c6a3834b5cde23daab066dfbfbbd5438b/prompt_toolkit-3.0.20-py3-none-any.whl\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->unityagents==0.4.0) (0.7.4)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->unityagents==0.4.0) (4.3.1)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->unityagents==0.4.0) (0.8.1)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->unityagents==0.4.0) (0.10.2)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->unityagents==0.4.0) (0.1.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->unityagents==0.4.0) (4.0.11)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from jinja2->notebook->jupyter->unityagents==0.4.0) (1.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.6/site-packages (from nbformat->notebook->jupyter->unityagents==0.4.0) (2.6.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0->ipykernel->unityagents==0.4.0) (0.5.2)\n",
      "Building wheels for collected packages: unityagents\n",
      "  Running setup.py bdist_wheel for unityagents ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-8rn14m_h/wheels/97/7a/24/09937717b9737178ae827bcef33ba219b540efd55be210010c\n",
      "Successfully built unityagents\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.20 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tensorflow, qtpy, qtconsole, prompt-toolkit, jupyter-console, jupyter, docopt, unityagents, widgetsnbextension\n",
      "  Found existing installation: prompt-toolkit 1.0.15\n",
      "    Uninstalling prompt-toolkit-1.0.15:\n",
      "      Successfully uninstalled prompt-toolkit-1.0.15\n",
      "  Found existing installation: widgetsnbextension 3.1.0\n",
      "    Uninstalling widgetsnbextension-3.1.0:\n",
      "      Successfully uninstalled widgetsnbextension-3.1.0\n",
      "Successfully installed docopt-0.6.2 jupyter-1.0.0 jupyter-console-6.4.0 prompt-toolkit-3.0.20 qtconsole-5.1.1 qtpy-1.11.1 tensorflow-1.7.1 unityagents-0.4.0 widgetsnbextension-3.0.8\n"
     ]
    }
   ],
   "source": [
    "# Jedi Not Working\n",
    "#  %config Completer.use_jedi = False\n",
    "! pip install ../python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### 4.1.1 Project Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Architecture of this project was extended from the model archiecture used \n",
    "in project 1. Primarily, I built an environment manager API such that the minor\n",
    "differences between OpenGym AI environments and Unity Environments can be\n",
    "abstracted from the Trainer and Agent models such that either environment can\n",
    "be used to train an agent with minimal to no change in code.\n",
    "\n",
    "This leads to the discussion of the three primary classes within this project.\n",
    "the `EnvironmentMgr`, the `Trainer`, and the `Agent` interfaces.\n",
    "\n",
    "1. `EnvironmentMgr` - Each `EnvironmentMgr` class contains common commands the\n",
    "   `Trainer` can interface with to command the environment to `start`, `step`,\n",
    "   `reset`, `get_evn`, and `close`.\n",
    "   \n",
    "2. `Trainer` - This class is intended to hold all of the properties for the\n",
    "   experiment and manipulate both the `Agent` and the `Environment`.\n",
    "   \n",
    "3. `Agent` - This is the class that holds the reinforement learning agent and\n",
    "   manitains a similar structure to other implementations with minor edits for\n",
    "   funciton encapsulation.\n",
    "\n",
    "#### 4.1.1.1 Agent Selection\n",
    "\n",
    "Due to the difficulty of the problem and the ammount of implementations using\n",
    "Deep Deterministic Policy Gradient (DDPG), I chose to implement a similar\n",
    "version, in order to leverage and compare my code with the available \n",
    "resources - and solicit feedback for others to review my code.\n",
    "\n",
    "Using the DDPG implementation from the Bipedal and Pendulum models as starting\n",
    "points I implemented my version of the DDPG agent. I implemented the \n",
    "Ornstein-Uhlenbeck process to add noise to my model similar to the example, and\n",
    "following the advice of the prompt - I implemented methods to restrict learning\n",
    "for the target Actor and Critic models as well as implementing a way to \n",
    "randomly sample a subset of agents (if n>1) for learning.\n",
    "\n",
    "#### 4.1.1.2 Neural Network Model Architecture\n",
    "\n",
    "After reviewing several times with fellow students and discussing with mentors\n",
    "within the forums. I've selected an `Actor` Model consisting of `4` fully \n",
    "connected layers with hidden layers of `256`, `128`, and `64` units wide and input\n",
    "units equal to the state size and output units equal to the action size. For\n",
    "the `Critic` Model, I've constructed a `4` fully connected model again with\n",
    "hidden layers equal to `256`, `128`, and `64` units wide, but following the \n",
    "recommendation of Agents of this structure to inject the states as inputs into\n",
    "the first layer and actions into the second. Finally, outputting a single node.\n",
    "\n",
    "For activation functions, the `ReLU` function was used to minimize complexity and\n",
    "the hyberbolic tangent function (`tanh`) was used as output for the `Actor`.\n",
    "\n",
    "Weights were initialized using uniform distribution from \n",
    "$\\mp\\frac{1}{\\sqrt{N_{input}}}$ for all of the nodes save for the final node where\n",
    "a uniform distribution between $\\mp3e-3$ was established."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Primary Import and Utility Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.20 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# %config Completer.use_jedi = True\n",
    "!pip -q install toml\n",
    "!pip install ../python\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from reacher_agents.ddpg_agent import DDPGAgent\n",
    "from reacher_agents.trainers import MultiAgentTrainer, SingleAgentTrainer\n",
    "\n",
    "ENV_TYPE = 'unity'      # enum ('unity', 'gym') = choose which environment to run\n",
    "CLOUD = True            # True if running in Udacity venv\n",
    "BUFFER_SIZE = int(1e6)  # Replay buffer size\n",
    "BATCH_SIZE = 16         # minibatch size\n",
    "N_EPISODES = 1000       # 300|3000 max number of episodes to run\n",
    "MAX_T = 1000            # Max time steps within an episode\n",
    "N_WORKERS = 20          # number of workers to run in environment\n",
    "MAX_WORKERS = 10        # number of workers to learn from an episode, ignored if N_WORKERS < MAX_WORKERS\n",
    "LEARN_F = 20            # Learning Frequency within epiodes\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # soft update target parameter\n",
    "LR_ACTOR = 1e-4         # learning rate for the actor\n",
    "LR_CRITIC = 1e-4        # learning rate for the critic\n",
    "WEIGHT_DECAY = 0.       #0.0001 - L2 weight decay parameter\n",
    "WINDOW_LEN = 100        # window length for averaging\n",
    "ACTOR_HIDDEN = (256, 128)\n",
    "CRITIC_HIDDEN = (256, 128)\n",
    "ADD_NOISE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if ENV_TYPE.lower() == 'gym':\n",
    "        import gym\n",
    "        from reacher_agents.gym_environments import GymContinuousEnvMgr\n",
    "    #     scenarios = {'LunarLanderContinuous-v2',\n",
    "    #                  'BipedalWalker-v3',\n",
    "    #                  'Pendulum-v0'}\n",
    "        envh = GymContinuousEnvMgr('Pendulum-v0')\n",
    "        root_name = 'gym'\n",
    "        Trainer = SingleAgentTrainer\n",
    "        upper_bound = 2.0\n",
    "        solved = -250\n",
    "    else:\n",
    "        from reacher_agents.unity_environments import UnityEnvMgr\n",
    "        root_name = 'unity'\n",
    "        if N_WORKERS == 1:\n",
    "            file_name = 'envs/Reacher_Windows_x86_64-one-agent/Reacher.exe'\n",
    "        else:\n",
    "            file_name = 'envs/Reacher_Windows_x86_64-twenty-agents/Reacher.exe'\n",
    "        envh = UnityEnvMgr(file_name)\n",
    "        Trainer = MultiAgentTrainer\n",
    "        upper_bound = 1.0\n",
    "        solved = 30.0\n",
    "\n",
    "    if CLOUD:\n",
    "        if N_WORKERS==1:\n",
    "            file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64'\n",
    "        else:\n",
    "            file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64'\n",
    "        envh = UnityEnvMgr(file_name)\n",
    "    env = envh.start()\n",
    "    state_size = envh.state_size\n",
    "    action_size = envh.action_size\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # device = torch.device(\"cpu\")\n",
    "    agent = DDPGAgent(\n",
    "        state_size=state_size,\n",
    "        action_size=action_size,\n",
    "        buffer_size=BUFFER_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        gamma=GAMMA,\n",
    "        tau=TAU,\n",
    "        lr_actor=LR_ACTOR,\n",
    "        lr_critic=LR_CRITIC,\n",
    "        learn_f=LEARN_F,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        device=device,\n",
    "        random_seed=42,\n",
    "        upper_bound=upper_bound,\n",
    "        actor_hidden=ACTOR_HIDDEN,\n",
    "        critic_hidden=CRITIC_HIDDEN,\n",
    "        add_nose=ADD_NOISE\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        agent=agent,\n",
    "        env=envh,\n",
    "        n_episodes=N_EPISODES,\n",
    "        max_t=MAX_T,\n",
    "        window_len=WINDOW_LEN,\n",
    "        solved=solved,\n",
    "        n_workers=N_WORKERS,\n",
    "        max_workers=MAX_WORKERS,  # note can be lower than n\n",
    "        save_root=root_name,\n",
    "    )\n",
    "    return envh, agent, trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### 4.1.4 Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section investigates learning rates of the `Actor` and `Critic` models as\n",
    "well as learning frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conducting a grid search with learning rate for actor and critic I the \n",
    "following relations running 50 episode epochs:\n",
    "```\n",
    "Actor LR: 1.0e-04\tCritic LR: 1.0e-04\n",
    "Episode 50\tAverage Score: 0.87\n",
    "\n",
    "Actor LR: 1.0e-04\tCritic LR: 1.0e-03\n",
    "Episode 50\tAverage Score: 0.47\n",
    "\n",
    "Actor LR: 1.0e-04\tCritic LR: 2.0e-03\n",
    "Episode 50\tAverage Score: 0.36\n",
    "\n",
    "Actor LR: 1.0e-03\tCritic LR: 1.0e-04\n",
    "Episode 50\tAverage Score: 0.86\n",
    "\n",
    "Actor LR: 1.0e-03\tCritic LR: 1.0e-03\n",
    "Episode 50\tAverage Score: 0.86\n",
    "\n",
    "Actor LR: 1.0e-03\tCritic LR: 2.0e-03\n",
    "Episode 50\tAverage Score: 0.04\n",
    "\n",
    "Actor LR: 2.0e-03\tCritic LR: 1.0e-04\n",
    "Episode 50\tAverage Score: 0.85\n",
    "\n",
    "Actor LR: 2.0e-03\tCritic LR: 1.0e-03\n",
    "Episode 50\tAverage Score: 0.04\n",
    "\n",
    "Actor LR: 2.0e-03\tCritic LR: 2.0e-03\n",
    "Episode 50\tAverage Score: 0.65\n",
    "```\n",
    "\n",
    "The fastest learning rates seem to be 1e-3 and 1e-4 for the actor and from \n",
    "1e-4 to 2e-3 for the critic. With LRs close to one another I found the best\n",
    "performance.\n",
    "I will select the learning rates `2e-3` and `1e-4` for the actor and critic\n",
    "repsectively.\n",
    "\n",
    "When investigating learning period for soft updating the following was \n",
    "observed:\n",
    "```\n",
    "Actor LR: 2.0e-03\tCritic LR: 1.0e-04\tL_Period: 1\n",
    "Episode 50\tAverage Score: 0.86\n",
    "\n",
    "Actor LR: 2.0e-03\tCritic LR: 1.0e-04\tL_Period: 5\n",
    "Episode 50\tAverage Score: 0.77\n",
    "\n",
    "Actor LR: 2.0e-03\tCritic LR: 1.0e-04\tL_Period: 10\n",
    "Episode 50\tAverage Score: 0.44\n",
    "\n",
    "Actor LR: 2.0e-03\tCritic LR: 1.0e-04\tL_Period: 15\n",
    "Episode 50\tAverage Score: 0.66\n",
    "\n",
    "Actor LR: 2.0e-03\tCritic LR: 1.0e-04\tL_Period: 20\n",
    "Episode 50\tAverage Score: 0.66\n",
    "```\n",
    "\n",
    "The fastest learning rate was inversely proportional to the period. Keeping\n",
    "in mind that I likly will need to maintain a period of `20` time steps taking\n",
    "into account previous advice from Udacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5 Run for Record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing implementations from other students as well reviewing comments from\n",
    "the Mentor Advice board - I've constructed the following Hyper Parameters for\n",
    "the run for record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'actor_hidden'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-906537193a1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menvh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-49b149fe2159>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mactor_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mACTOR_HIDDEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mcritic_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCRITIC_HIDDEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0madd_nose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mADD_NOISE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     54\u001b[0m     trainer = Trainer(\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'actor_hidden'"
     ]
    }
   ],
   "source": [
    "envh, agent, trainer = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.agent.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.critic_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.6 Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the scores of your trained agent. \n",
    "\n",
    "* The `i_map` parameter to rotate through the seaborn color palette (paired in \n",
    "  groups of 2)\n",
    "  * 0: blue\n",
    "  * 1: green\n",
    "  * 2: red\n",
    "  * 3: orange\n",
    "  * 4: purple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(trainer, i_map=0):\n",
    "    sns.set_style('darkgrid')\n",
    "    sns.set_context('talk')\n",
    "    sns.set_palette('Paired')\n",
    "    cmap = sns.color_palette('Paired')\n",
    "    if trainer.n_workers > 1:\n",
    "        scores = np.mean(np.array(trainer.scores_).squeeze(), 1)\n",
    "    else:\n",
    "        scores = np.array(trainer.scores_).squeeze()\n",
    "    alr, clr, lf = trainer.agent.lr_actor, trainer.agent.lr_critic, trainer.agent.learn_f\n",
    "    score_df = pd.DataFrame({'scores': scores})\n",
    "    score_df = score_df.assign(mean=lambda df: df.rolling(10).mean()['scores'])\n",
    "\n",
    "    fig ,ax = plt.subplots(1,1, figsize=(10,8))\n",
    "\n",
    "    ax = score_df.plot(ax=ax, color=cmap[2*(i_map%4):])\n",
    "    ax.set_title(f'DDPG Scores vs Time (LR=({alr:.1e}, {clr:.1e}), Lf={lf})')\n",
    "    ax.set_xlabel('Episode #')\n",
    "    ax.set_ylabel('Score')\n",
    "    plt.show()\n",
    "plot_scores(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.6 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load(\n",
    "    r'D:\\udacity\\deep-rl\\projects/p2_reacher/cont-control/multi-checkpoint_actor-7.5.pth',\n",
    "    r'D:\\udacity\\deep-rl\\projects/p2_reacher/cont-control/multi-checkpoint_critic-7.5.pth',\n",
    ")\n",
    "etrainer = Trainer(\n",
    "    agent=agent,\n",
    "    env=envh,\n",
    "    n_workers=N_WORKERS,\n",
    "    n_episodes=100,\n",
    "    max_t=1000,\n",
    "    window_len=100,\n",
    "    solved=30.0,\n",
    "    max_workers=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = etrainer.eval(n_episodes=100, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(etrainer, i_map=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "envh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Results\n",
    "\n",
    "As shown above the DDPG implementation provides consistent, albeit slow \n",
    "learning. The Model was able to solve the environment using `20` agents in \n",
    "`TBR` epsiodes. Much slower than what was demonstrated in the problem prompt.\n",
    "The slow learning rate and reducing soft updating to every `20` steps \n",
    "contributed to this rate. However, increasing learning rates demonstrated \n",
    "erratic or poor performance at low episode levels (<50). Clearly, more tuning\n",
    "can improve this learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training for this particular agent is very slow - further tuning of the \n",
    "hyper parameters should improve efficiency. However, applying newer \n",
    "Actor/Critic models such as Twin Delayed DDPG (TD3) would be a direct \n",
    "improvement over the applied DDPG application. Another avenue to explore would\n",
    "be to investigate an on-policy method such as Asynchronous Actor Critic (A3C)\n",
    "to evaluate performance directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15e440525741b4b683536cf3da481e83dd46ee608086a17f4cdb272058241969"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
